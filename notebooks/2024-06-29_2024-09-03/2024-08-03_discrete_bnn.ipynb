{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from path_info import PROJECT_DIR\n",
    "sys.path.append(PROJECT_DIR)\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.utils_benchmark_functions_2 import test_function\n",
    "from src.utils_benchmark_functions import discretize_function\n",
    "from src.botorch_wrapper import DiscreteBO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "from botorch.models.model import Model\n",
    "from botorch.acquisition import UpperConfidenceBound\n",
    "from botorch.optim import optimize_acqf\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "class BayesianLinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(BayesianLinearRegression, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Parameters for the prior distributions of weights and biases\n",
    "        self.w_mu = nn.Parameter(torch.zeros(input_dim, output_dim))\n",
    "        self.w_log_sigma = nn.Parameter(torch.zeros(input_dim, output_dim))\n",
    "        self.b_mu = nn.Parameter(torch.zeros(output_dim))\n",
    "        self.b_log_sigma = nn.Parameter(torch.zeros(output_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        w_sigma = torch.exp(self.w_log_sigma)\n",
    "        b_sigma = torch.exp(self.b_log_sigma)\n",
    "        \n",
    "        # Sampling weights and biases\n",
    "        w = self.w_mu + w_sigma * torch.randn_like(self.w_mu)\n",
    "        b = self.b_mu + b_sigma * torch.randn_like(self.b_mu)\n",
    "        \n",
    "        return torch.matmul(x, w) + b\n",
    "    \n",
    "    def predict_dist(self, x):\n",
    "        y = self.forward(x)\n",
    "        \n",
    "        # Calculating the uncertainty of the output\n",
    "        w_sigma = torch.exp(self.w_log_sigma)\n",
    "        b_sigma = torch.exp(self.b_log_sigma)\n",
    "        \n",
    "        # Calculating the standard deviation considering the uncertainty of weights and biases\n",
    "        output_sigma = torch.sqrt(torch.matmul(x**2, w_sigma**2) + b_sigma**2)\n",
    "        \n",
    "        return Normal(y, output_sigma)\n",
    "\n",
    "\n",
    "class BayesianMLP(nn.Module):\n",
    "    def __init__(self, input_dim, min_val=None, max_val=None):\n",
    "        super(BayesianMLP, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_dim, 64)\n",
    "        self.hidden2 = nn.Linear(64, 64)\n",
    "        self.hidden3 = nn.Linear(64, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bayesian_output = BayesianLinearRegression(64, 1)\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.relu(self.hidden3(x))\n",
    "        \n",
    "        # Get output from Bayesian linear regression\n",
    "        y_dist = self.bayesian_output.predict_dist(x)\n",
    "\n",
    "        if self.min_val is not None or self.max_val is not None:\n",
    "            # Clamp the output\n",
    "            y_mean = torch.clamp(y_dist.mean, min=self.min_val, max=self.max_val)\n",
    "        else:\n",
    "            y_mean = y_dist.mean\n",
    "\n",
    "        y_stddev = y_dist.stddev  # Keep the standard deviation as it is\n",
    "        \n",
    "        # Return new distribution\n",
    "        return Normal(y_mean, y_stddev)\n",
    "\n",
    "\n",
    "class BayesianMLPModel(Model):\n",
    "    def __init__(self, input_dim, min_val=None, max_val=None):\n",
    "        super().__init__()\n",
    "        self.bayesian_mlp = BayesianMLP(input_dim, min_val, max_val)\n",
    "        self.likelihood = GaussianLikelihood()\n",
    "        self._num_outputs = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bayesian_mlp(x)\n",
    "    \n",
    "    def posterior(self, X, observation_noise=False, **kwargs):\n",
    "        pred_dist = self.bayesian_mlp(X)\n",
    "        mean = pred_dist.mean.squeeze(-1)  # Ensure mean is 2D\n",
    "        stddev = pred_dist.stddev.squeeze(-1)  # Ensure stddev is 2D\n",
    "        covar = torch.diag_embed(stddev**2)\n",
    "        return MultivariateNormal(mean, covar)\n",
    "    \n",
    "    @property\n",
    "    def num_outputs(self):\n",
    "        return self._num_outputs\n",
    "    \n",
    "    @property\n",
    "    def train_inputs(self):\n",
    "        return self._train_inputs\n",
    "\n",
    "    @property\n",
    "    def train_targets(self):\n",
    "        return self._train_targets\n",
    "\n",
    "    def set_train_data(self, inputs=None, targets=None, strict=True):\n",
    "        self._train_inputs = inputs\n",
    "        self._train_targets = targets\n",
    "\n",
    "\n",
    "def train_model(model, train_X, train_Y, num_epochs=1000, learning_rate=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output_dist = model(train_X)\n",
    "        loss = -output_dist.log_prob(train_Y).mean()  # Minimize negative log likelihood\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X: tensor([[2.4328],\n",
      "        [3.6670],\n",
      "        [3.3378],\n",
      "        [1.9746],\n",
      "        [3.2818],\n",
      "        [3.1092],\n",
      "        [3.1636],\n",
      "        [2.4338],\n",
      "        [3.5061],\n",
      "        [1.3407]])\n",
      "train_Y: tensor([[1.8732e-01],\n",
      "        [2.7788e+00],\n",
      "        [1.7897e+00],\n",
      "        [6.4470e-04],\n",
      "        [1.6429e+00],\n",
      "        [1.2303e+00],\n",
      "        [1.3539e+00],\n",
      "        [1.8821e-01],\n",
      "        [2.2683e+00],\n",
      "        [4.3472e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Define the test function\n",
    "def test_function(x):\n",
    "    if 0 <= x <= 1:\n",
    "        return 10 + 10 * (x - 0.5)**2  # High penalty within [0, 1]\n",
    "    else:\n",
    "        return (x - 2)**2  # Quadratic function with a minimum at x = 2\n",
    "\n",
    "\n",
    "# Convert test_function to a PyTorch tensor\n",
    "def objective(x):\n",
    "    return torch.tensor([test_function(xi.item()) for xi in x], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Create initial data points\n",
    "train_X = torch.rand(10, 1) * 4  # Generate 10 random initial points in the range [0, 4)\n",
    "train_Y = torch.tensor([test_function(xi.item()) for xi in train_X], dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "print(f'train_X: {train_X}')\n",
    "print(f'train_Y: {train_Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteBO():\n",
    "    def __init__(self, bounds, beta=2.0, beta_h=10.0, l_h=2.0):\n",
    "        super().__init__(bounds)\n",
    "        self.beta = beta\n",
    "        self.beta_h = beta_h\n",
    "        self.l_h = l_h\n",
    "        self.l = 1.0\n",
    "\n",
    "    def initialize(self, X_init, Y_init):\n",
    "        self.train_X = X_init\n",
    "        self.train_Y = Y_init\n",
    "        self.model = self._fit_model()\n",
    "\n",
    "    # def _fit_model(self):\n",
    "    #     model = SingleTaskGP(self.train_X, self.train_Y)\n",
    "    #     mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    #     fit_gpytorch_mll(mll)\n",
    "    #     return model\n",
    "\n",
    "    def _fit_model(self):\n",
    "        # Define and fit the model\n",
    "        model = BayesianMLPModel(input_dim=1)\n",
    "        model.set_train_data(self.train_X, self.train_Y)\n",
    "        model = train_model(model, self.train_X, self.train_Y)\n",
    "        return model\n",
    "\n",
    "    def acquisition_function(self, beta):\n",
    "        return UpperConfidenceBound(self.model, beta=beta)\n",
    "\n",
    "    def optimize_acquisition(self, acq_function):\n",
    "        candidates, _ = optimize_acqf(\n",
    "            acq_function,\n",
    "            bounds=self.bounds,\n",
    "            q=1,\n",
    "            num_restarts=10,\n",
    "            raw_samples=20,\n",
    "        )\n",
    "        return candidates\n",
    "\n",
    "    def step(self):\n",
    "        acq_function = self.acquisition_function(self.beta)\n",
    "        new_X = self.optimize_acquisition(acq_function)\n",
    "\n",
    "        new_X = torch.round(new_X)\n",
    "\n",
    "        # Check if new_X is in train_X and adjust beta and l if necessary\n",
    "        while (new_X == self.train_X).all(dim=1).any():\n",
    "            self.adjust_beta_and_l()\n",
    "            acq_function = self.acquisition_function(self.beta)\n",
    "            new_X = self.optimize_acquisition(acq_function)\n",
    "            new_X = torch.round(new_X)\n",
    "\n",
    "        return new_X\n",
    "\n",
    "    def update(self, new_X, new_Y):\n",
    "        self.train_X = torch.cat([self.train_X, new_X], dim=0)\n",
    "        self.train_Y = torch.cat([self.train_Y, new_Y], dim=0)\n",
    "        self.model = self._fit_model()\n",
    "\n",
    "    def adjust_beta_and_l(self):\n",
    "        # Define the optimization problem to adjust beta and l\n",
    "        def objective(params):\n",
    "            delta_beta, l = params\n",
    "            adjusted_beta = self.beta + delta_beta\n",
    "            acq_function = self.acquisition_function(adjusted_beta)\n",
    "            new_X = self.optimize_acquisition(acq_function)\n",
    "            rounded_new_X = torch.round(new_X)\n",
    "\n",
    "            penalty = float('inf')\n",
    "            if (rounded_new_X == self.train_X).all(dim=1).any():\n",
    "                penalty = 1000  # Arbitrary high value to avoid repetition\n",
    "\n",
    "            return delta_beta + torch.norm(new_X - rounded_new_X).item() + penalty\n",
    "\n",
    "        # Initial guess and bounds for delta_beta and l\n",
    "        initial_guess = torch.tensor([0.0, self.l])\n",
    "        bounds = [(0.0, self.beta_h), (1e-3, self.l_h)]\n",
    "\n",
    "        # Optimize the objective function\n",
    "        result = minimize(objective, initial_guess, bounds=bounds, method='L-BFGS-B')\n",
    "        delta_beta, l = result.x\n",
    "        self.beta += delta_beta\n",
    "        self.l = l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Optimal candidate: 3.809638023376465\n",
      "Iteration 2, Optimal candidate: 0.2322077453136444\n",
      "Iteration 3, Optimal candidate: 4.0\n",
      "Iteration 4, Optimal candidate: 4.0\n",
      "Iteration 5, Optimal candidate: 3.9056220054626465\n",
      "Iteration 6, Optimal candidate: 2.9983606338500977\n",
      "Iteration 7, Optimal candidate: 3.6666674613952637\n",
      "Iteration 8, Optimal candidate: 0.0\n",
      "Iteration 9, Optimal candidate: 4.0\n",
      "Iteration 10, Optimal candidate: 1.810307264328003\n",
      "Iteration 11, Optimal candidate: 1.6404427289962769\n",
      "Iteration 12, Optimal candidate: 3.8088953495025635\n",
      "Iteration 13, Optimal candidate: 0.9856517314910889\n",
      "Iteration 14, Optimal candidate: 1.440605878829956\n",
      "Iteration 15, Optimal candidate: 3.4827260971069336\n",
      "Iteration 16, Optimal candidate: 2.7431647777557373\n",
      "Iteration 17, Optimal candidate: 0.19765116274356842\n",
      "Iteration 18, Optimal candidate: 3.5888752937316895\n",
      "Iteration 19, Optimal candidate: 2.6874797344207764\n",
      "Iteration 20, Optimal candidate: 1.287191390991211\n",
      "Iteration 21, Optimal candidate: 2.290058135986328\n",
      "Iteration 22, Optimal candidate: 3.2855591773986816\n",
      "Iteration 23, Optimal candidate: 3.5737392902374268\n",
      "Iteration 24, Optimal candidate: 2.3888978958129883\n",
      "Iteration 25, Optimal candidate: 3.5582351684570312\n",
      "Iteration 26, Optimal candidate: 2.338484764099121\n",
      "Iteration 27, Optimal candidate: 1.7965829372406006\n",
      "Iteration 28, Optimal candidate: 2.667384147644043\n",
      "Iteration 29, Optimal candidate: 2.3144187927246094\n",
      "Iteration 30, Optimal candidate: 2.0847887992858887\n",
      "Iteration 31, Optimal candidate: 0.40672963857650757\n",
      "Iteration 32, Optimal candidate: 0.5228139758110046\n",
      "Iteration 33, Optimal candidate: 1.9343295097351074\n",
      "Iteration 34, Optimal candidate: 0.35509932041168213\n",
      "Iteration 35, Optimal candidate: 1.6402239799499512\n",
      "Iteration 36, Optimal candidate: 0.8428438305854797\n",
      "Iteration 37, Optimal candidate: 2.8438467979431152\n",
      "Iteration 38, Optimal candidate: 3.1253230571746826\n",
      "Iteration 39, Optimal candidate: 0.0016635097563266754\n",
      "Iteration 40, Optimal candidate: 1.7677888870239258\n",
      "Iteration 41, Optimal candidate: 1.546432375907898\n",
      "Iteration 42, Optimal candidate: 2.4671261310577393\n",
      "Iteration 43, Optimal candidate: 1.6869341135025024\n",
      "Iteration 44, Optimal candidate: 0.11151885241270065\n",
      "Iteration 45, Optimal candidate: 2.6665544509887695\n",
      "Iteration 46, Optimal candidate: 1.0691132545471191\n",
      "Iteration 47, Optimal candidate: 2.395674705505371\n",
      "Iteration 48, Optimal candidate: 4.0\n",
      "Iteration 49, Optimal candidate: 0.38996073603630066\n",
      "Iteration 50, Optimal candidate: 3.70286226272583\n",
      "Iteration 51, Optimal candidate: 2.1648073196411133\n",
      "Iteration 52, Optimal candidate: 3.018974781036377\n",
      "Iteration 53, Optimal candidate: 1.3014841079711914\n",
      "Iteration 54, Optimal candidate: 3.171945333480835\n",
      "Iteration 55, Optimal candidate: 0.6353138089179993\n",
      "Iteration 56, Optimal candidate: 0.4289352595806122\n",
      "Iteration 57, Optimal candidate: 2.2919793128967285\n",
      "Iteration 58, Optimal candidate: 0.5388861298561096\n",
      "Iteration 59, Optimal candidate: 0.09458422660827637\n",
      "Iteration 60, Optimal candidate: 0.0\n",
      "Iteration 61, Optimal candidate: 0.7433988451957703\n",
      "Iteration 62, Optimal candidate: 1.8931955099105835\n",
      "Iteration 63, Optimal candidate: 3.9639785289764404\n",
      "Iteration 64, Optimal candidate: 1.6735732555389404\n",
      "Iteration 65, Optimal candidate: 1.1519200801849365\n",
      "Iteration 66, Optimal candidate: 0.4795845150947571\n",
      "Iteration 67, Optimal candidate: 2.540400743484497\n",
      "Iteration 68, Optimal candidate: 1.2890795469284058\n",
      "Iteration 69, Optimal candidate: 2.879207134246826\n",
      "Iteration 70, Optimal candidate: 3.274388313293457\n",
      "Iteration 71, Optimal candidate: 0.953167736530304\n",
      "Iteration 72, Optimal candidate: 3.9813225269317627\n",
      "Iteration 73, Optimal candidate: 3.3040528297424316\n",
      "Iteration 74, Optimal candidate: 2.5414810180664062\n",
      "Iteration 75, Optimal candidate: 3.6831490993499756\n",
      "Iteration 76, Optimal candidate: 0.45780593156814575\n",
      "Iteration 77, Optimal candidate: 0.10394442081451416\n",
      "Iteration 78, Optimal candidate: 3.1022679805755615\n",
      "Iteration 79, Optimal candidate: 3.738053560256958\n",
      "Iteration 80, Optimal candidate: 0.7601550221443176\n",
      "Iteration 81, Optimal candidate: 1.2622027397155762\n",
      "Iteration 82, Optimal candidate: 3.9150502681732178\n",
      "Iteration 83, Optimal candidate: 2.4417929649353027\n",
      "Iteration 84, Optimal candidate: 2.911830186843872\n",
      "Iteration 85, Optimal candidate: 1.4921175241470337\n",
      "Iteration 86, Optimal candidate: 3.637817859649658\n",
      "Iteration 87, Optimal candidate: 2.583578109741211\n",
      "Iteration 88, Optimal candidate: 2.7185299396514893\n",
      "Iteration 89, Optimal candidate: 0.6499914526939392\n",
      "Iteration 90, Optimal candidate: 2.668438673019409\n",
      "Iteration 91, Optimal candidate: 3.4866061210632324\n",
      "Iteration 92, Optimal candidate: 3.6105220317840576\n",
      "Iteration 93, Optimal candidate: 3.5792956352233887\n",
      "Iteration 94, Optimal candidate: 2.03926420211792\n",
      "Iteration 95, Optimal candidate: 3.221215009689331\n",
      "Iteration 96, Optimal candidate: 1.6082185506820679\n",
      "Iteration 97, Optimal candidate: 1.2590535879135132\n",
      "Iteration 98, Optimal candidate: 1.9730074405670166\n",
      "Iteration 99, Optimal candidate: 0.17368951439857483\n",
      "Iteration 100, Optimal candidate: 1.6928097009658813\n",
      "\n",
      "\n",
      "\n",
      "hellooooo\n",
      "tensor([1.9746])\n"
     ]
    }
   ],
   "source": [
    "# Optimization loop\n",
    "for i in range(100):\n",
    "    # Define and fit the model\n",
    "    model = BayesianMLPModel(input_dim=1)\n",
    "    model.set_train_data(train_X, train_Y)\n",
    "    model = train_model(model, train_X, train_Y)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Define the acquisition function\n",
    "    acq_func = UpperConfidenceBound(model, beta=0.1)\n",
    "    \n",
    "    # Perform optimization\n",
    "    bounds = torch.tensor([[0.0], [4.0]])\n",
    "    candidate, acq_value = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=1,\n",
    "        num_restarts=5,\n",
    "        raw_samples=20,\n",
    "    )\n",
    "    \n",
    "    print(f\"Iteration {i+1}, Optimal candidate: {candidate.item()}\")\n",
    "    \n",
    "    # Obtain new data point\n",
    "    new_x = candidate\n",
    "    new_y = torch.tensor([test_function(new_x.item())], dtype=torch.float32).unsqueeze(-1)\n",
    "    \n",
    "    # Update data\n",
    "    train_X = torch.cat([train_X, new_x])\n",
    "    train_Y = torch.cat([train_Y, new_y])\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(\"hellooooo\")\n",
    "print(train_X[train_Y.argmin().item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys\n",
    "from path_info import PROJECT_DIR\n",
    "sys.path.append(PROJECT_DIR)\n",
    "\n",
    "from src.bnn import BayesianMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのURL\n",
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# 特徴量とターゲットの分割\n",
    "X = df.drop(columns=['medv']).values\n",
    "y = df['medv'].values\n",
    "\n",
    "# データの分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 特徴量の標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# PyTorchテンソルへの変換\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.clamp: At least one of 'min' or 'max' must not be None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.clamp: At least one of 'min' or 'max' must not be None"
     ]
    }
   ],
   "source": [
    "# モデルの定義とオプティマイザの設定\n",
    "model = BayesianMLP(input_dim=X_train.shape[1], min_val=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# トレーニング\n",
    "model.train()\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output_dist = model(X_train)\n",
    "    loss = -output_dist.log_prob(y_train).mean()  # 負の対数尤度を最小化\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# トレーニングデータでの予測\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_output_dist = model(X_train)\n",
    "    train_y_pred = train_output_dist.mean\n",
    "\n",
    "# テストデータでの予測\n",
    "with torch.no_grad():\n",
    "    test_output_dist = model(X_test)\n",
    "    test_y_pred = test_output_dist.mean\n",
    "\n",
    "# 予測誤差の計算\n",
    "mse = nn.MSELoss()\n",
    "train_mse = mse(train_y_pred, y_train).item()\n",
    "test_mse = mse(test_y_pred, y_test).item()\n",
    "\n",
    "print('\\n-------------------------------------------------')\n",
    "print(f'Training Mean Squared Error: {train_mse:.4f}')\n",
    "print(f'Test Mean Squared Error: {test_mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
