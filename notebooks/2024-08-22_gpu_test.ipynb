{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 結論\n",
        "\n",
        "データ点数，素子数と層数（パラメータ数），データの次元数に比例して計算量が多くなり，GPU の優位性が上がっていく\n",
        "\n",
        "ベイズ最適化は基本的にデータ点数が少ないため，CPU での計算で十分である場合がある（ユニット数 64, 潜在層 3 程度） "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZz7GaAc68cr",
        "outputId": "79402277-d033-40d4-f382-b8f26e766265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Using cached pip-24.2-py3-none-any.whl (1.8 MB)\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.2\n",
            "Requirement already satisfied: botorch in /usr/local/lib/python3.10/dist-packages (0.11.3)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from botorch) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from botorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.10/dist-packages (from botorch) (1.3.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from botorch) (2.3.1+cu121)\n",
            "Requirement already satisfied: pyro-ppl>=1.8.4 in /usr/local/lib/python3.10/dist-packages (from botorch) (1.9.1)\n",
            "Requirement already satisfied: gpytorch==1.12 in /usr/local/lib/python3.10/dist-packages (from botorch) (1.12)\n",
            "Requirement already satisfied: linear-operator==0.5.2 in /usr/local/lib/python3.10/dist-packages (from botorch) (0.5.2)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from botorch) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch==1.12->botorch) (1.3.2)\n",
            "Requirement already satisfied: jaxtyping>=0.2.9 in /usr/local/lib/python3.10/dist-packages (from linear-operator==0.5.2->botorch) (0.2.33)\n",
            "Requirement already satisfied: typeguard~=2.13.3 in /usr/local/lib/python3.10/dist-packages (from linear-operator==0.5.2->botorch) (2.13.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl>=1.8.4->botorch) (3.3.0)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl>=1.8.4->botorch) (0.1.2)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl>=1.8.4->botorch) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->botorch) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.1->botorch) (2.1.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch==1.12->botorch) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch==1.12->botorch) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip\n",
        "!pip install botorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HvldCkhY6Xd1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from botorch.optim import optimize_acqf\n",
        "from botorch.acquisition import UpperConfidenceBound\n",
        "from scipy.optimize import minimize\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yMql4o9i6Xgd"
      },
      "outputs": [],
      "source": [
        "def get_opposite(direction: str) -> str:\n",
        "    \"\"\"\n",
        "    Examples\n",
        "    --------\n",
        "    >>> get_opposite('a')\n",
        "    'c'\n",
        "    \"\"\"\n",
        "    pair_dict = {\"a\": \"c\", \"c\": \"a\", \"b\": \"d\", \"d\": \"b\"}\n",
        "    return pair_dict.get(direction, \"\")\n",
        "\n",
        "\n",
        "def judge_continuity(d_from: str, current_direction: str) -> bool:\n",
        "    \"\"\"\n",
        "    Examples\n",
        "    --------\n",
        "    >>> judge_continuity('a', 'ad')\n",
        "    False\n",
        "    >>> judge_continuity('a', 'bc')\n",
        "    True\n",
        "    \"\"\"\n",
        "    d_opposite = get_opposite(d_from)\n",
        "    return d_opposite in current_direction\n",
        "\n",
        "\n",
        "def get_next_coordinate(\n",
        "    d_to: str, current_coordinate: tuple[int, int]\n",
        ") -> tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Examples\n",
        "    --------\n",
        "    >>> get_next_coordinate('a', (0, 0))\n",
        "    (-1, 0)\n",
        "    \"\"\"\n",
        "    update_dict = {\"a\": (-1, 0), \"b\": (0, -1), \"c\": (0, 1), \"d\": (1, 0)}\n",
        "    delta = update_dict.get(d_to, (0, 0))\n",
        "    return (current_coordinate[0] + delta[0], current_coordinate[1] + delta[1])\n",
        "\n",
        "\n",
        "def judge_location_validity(current: tuple[int, int], shape: tuple[int, int]) -> bool:\n",
        "    \"\"\"\n",
        "    Examples\n",
        "    --------\n",
        "    >>> judge_location_validity((-1, 0), (3, 3))\n",
        "    False\n",
        "    >>> judge_location_validity((1, 2), (3, 3))\n",
        "    True\n",
        "    \"\"\"\n",
        "    return 0 <= current[0] < shape[0] and 0 <= current[1] < shape[1]\n",
        "\n",
        "\n",
        "def get_d_to(d_from: str, current_direction: str) -> str:\n",
        "    \"\"\"\n",
        "    Examples\n",
        "    --------\n",
        "    >>> get_d_to('a', 'ad')\n",
        "    'd'\n",
        "    \"\"\"\n",
        "    return (\n",
        "        current_direction[1] if current_direction[0] == d_from else current_direction[0]\n",
        "    )\n",
        "\n",
        "\n",
        "def navigate_through_matrix(direction_matrix, start, goal):\n",
        "    history = []\n",
        "    current = start\n",
        "    shape = direction_matrix.shape\n",
        "\n",
        "    if direction_matrix[current] != \"bd\":\n",
        "        return history\n",
        "\n",
        "    history.append(current)\n",
        "    d_to = \"d\"\n",
        "    next_pos = get_next_coordinate(d_to, current)\n",
        "\n",
        "    while judge_location_validity(next_pos, shape) and current != goal:\n",
        "        if not judge_continuity(d_to, direction_matrix[next_pos]):\n",
        "            break\n",
        "\n",
        "        current = next_pos\n",
        "        history.append(current)\n",
        "        if current == goal:\n",
        "            break\n",
        "\n",
        "        direction = direction_matrix[current]\n",
        "        d_from = get_opposite(d_to)\n",
        "        d_to = get_d_to(d_from, direction)\n",
        "        next_pos = get_next_coordinate(d_to, current)\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def manhattan_distance(coord1: tuple[int, int], coord2: tuple[int, int]) -> int:\n",
        "    \"\"\"\n",
        "    Examples\n",
        "    --------\n",
        "    >>> manhattan_distance((0, 0), (3, 3))\n",
        "    6\n",
        "    \"\"\"\n",
        "    return abs(coord1[0] - coord2[0]) + abs(coord1[1] - coord2[1])\n",
        "\n",
        "\n",
        "class WarcraftObjectiveBoTorch:\n",
        "    def __init__(\n",
        "        self,\n",
        "        weight_matrix: torch.tensor,\n",
        "    ) -> None:\n",
        "        self.weight_matrix = weight_matrix\n",
        "        self.shape = weight_matrix.shape\n",
        "        self.search_space_1d_dict = {\n",
        "            -3: \"oo\", -2: \"ab\", -1: \"ac\", 0: \"ad\", 1: \"bc\", 2: \"bd\", 3: \"cd\"\n",
        "        }\n",
        "        self.reverse_search_space_1d_dict = {v: k for k, v in self.search_space_1d_dict.items()}\n",
        "\n",
        "    def string_to_tensor(\n",
        "        self,\n",
        "        direction_matrix: np.ndarray\n",
        "    ) -> torch.tensor:\n",
        "        tensor_matrix = torch.zeros(self.shape, dtype=torch.int)\n",
        "        for i in range(self.shape[0]):\n",
        "            for j in range(self.shape[1]):\n",
        "                tensor_matrix[i, j] = self.reverse_search_space_1d_dict.get(direction_matrix[i, j], 0.0)\n",
        "        return torch.tensor(tensor_matrix, dtype=torch.float32)\n",
        "\n",
        "    def tensor_to_string(\n",
        "        self,\n",
        "        x: torch.tensor\n",
        "    ) -> np.ndarray:\n",
        "        direction_matrix = np.zeros(self.shape, dtype=object)\n",
        "        for i in range(self.shape[0]):\n",
        "            for j in range(self.shape[1]):\n",
        "                direction_matrix[i, j] = self.search_space_1d_dict.get(x[i, j].item())\n",
        "        return direction_matrix\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x: torch.tensor\n",
        "    ) -> torch.Tensor:\n",
        "        if type(x) == torch.Tensor:\n",
        "            direction_matrix = self.tensor_to_string(x)\n",
        "        else:\n",
        "            direction_matrix = x\n",
        "\n",
        "        start = (0, 0)\n",
        "        goal = (self.shape[0]-1, self.shape[1]-1)\n",
        "\n",
        "        history = navigate_through_matrix(direction_matrix, start, goal)\n",
        "\n",
        "        if history:\n",
        "            path_weight = sum(self.weight_matrix[point] for point in history)\n",
        "            norm_const = manhattan_distance(start, goal)\n",
        "            loss1 = 1 - (1 - manhattan_distance(history[-1], goal) / norm_const) + path_weight\n",
        "        else:\n",
        "            loss1 = 1\n",
        "\n",
        "        mask = direction_matrix != \"oo\"\n",
        "        loss2 = self.weight_matrix[mask].sum()\n",
        "\n",
        "        loss = loss1 + loss2\n",
        "        score = -loss\n",
        "\n",
        "        return score\n",
        "\n",
        "    def visualize(\n",
        "        self,\n",
        "        x: torch.tensor\n",
        "    ) -> None:\n",
        "        direction_matrix = self.tensor_to_string(x)\n",
        "        print(direction_matrix)\n",
        "\n",
        "\n",
        "def generate_initial_data(\n",
        "    objective_function: callable,\n",
        "    dataset_size: int,\n",
        "    shape: tuple[int, int],\n",
        ") -> torch.tensor:\n",
        "    values = torch.tensor([-3, -2, -1, 0, 1, 2, 3])\n",
        "    n, m = shape\n",
        "    X_train = values[torch.randint(0, len(values), (dataset_size, n, m))]\n",
        "    y_train = torch.stack([objective_function(x) for x in X_train]).unsqueeze(-1)\n",
        "    return X_train, y_train\n",
        "\n",
        "\n",
        "weight_matrix = torch.Tensor([\n",
        "    [0.1, 0.4, 0.8, 0.8],\n",
        "    [0.2, 0.4, 0.4, 0.8],\n",
        "    [0.8, 0.1, 0.1, 0.2],\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gPz2lD096Xiw"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.distributions.normal import Normal\n",
        "# from botorch.models.model import Model\n",
        "# from gpytorch.likelihoods import GaussianLikelihood\n",
        "# from gpytorch.distributions import MultivariateNormal\n",
        "\n",
        "# # Bayesian Linear Regression class\n",
        "# class BayesianLinearRegression(nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim):\n",
        "#         super(BayesianLinearRegression, self).__init__()\n",
        "#         self.input_dim = input_dim\n",
        "#         self.output_dim = output_dim\n",
        "\n",
        "#         # Parameters for prior distributions of weights and biases\n",
        "#         self.w_mu = nn.Parameter(torch.zeros(input_dim, output_dim))\n",
        "#         self.w_log_sigma = nn.Parameter(torch.zeros(input_dim, output_dim))\n",
        "#         self.b_mu = nn.Parameter(torch.zeros(output_dim))\n",
        "#         self.b_log_sigma = nn.Parameter(torch.zeros(output_dim))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         w_sigma = torch.exp(self.w_log_sigma)\n",
        "#         b_sigma = torch.exp(self.b_log_sigma)\n",
        "\n",
        "#         # Sample weights and biases\n",
        "#         w = self.w_mu + w_sigma * torch.randn_like(self.w_mu)\n",
        "#         b = self.b_mu + b_sigma * torch.randn_like(self.b_mu)\n",
        "\n",
        "#         return torch.matmul(x, w) + b\n",
        "\n",
        "#     def predict_dist(self, x):\n",
        "#         y = self.forward(x)\n",
        "\n",
        "#         # Compute uncertainty in the output\n",
        "#         w_sigma = torch.exp(self.w_log_sigma)\n",
        "#         b_sigma = torch.exp(self.b_log_sigma)\n",
        "\n",
        "#         # Calculate the standard deviation considering the uncertainty in weights and biases\n",
        "#         output_sigma = torch.sqrt(torch.matmul(x**2, w_sigma**2) + b_sigma**2)\n",
        "\n",
        "#         return Normal(y, output_sigma)\n",
        "\n",
        "# # Bayesian MLP class\n",
        "# class BayesianMLP(nn.Module):\n",
        "#     def __init__(self, input_dim, min_val=None, max_val=None, hidden_unit_size=64, clipping=False):\n",
        "#         super(BayesianMLP, self).__init__()\n",
        "#         self.hidden1 = nn.Linear(input_dim, hidden_unit_size)\n",
        "#         self.hidden2 = nn.Linear(hidden_unit_size, hidden_unit_size)\n",
        "#         self.hidden3 = nn.Linear(hidden_unit_size, hidden_unit_size)\n",
        "#         self.hidden4 = nn.Linear(hidden_unit_size, hidden_unit_size)\n",
        "#         self.hidden5 = nn.Linear(hidden_unit_size, hidden_unit_size)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.bayesian_output = BayesianLinearRegression(hidden_unit_size, 1)\n",
        "#         self.min_val = min_val\n",
        "#         self.max_val = max_val\n",
        "#         self.clipping = clipping\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.relu(self.hidden1(x))\n",
        "#         x = self.relu(self.hidden2(x))\n",
        "#         x = self.relu(self.hidden3(x))\n",
        "#         x = self.relu(self.hidden4(x))\n",
        "#         x = self.relu(self.hidden5(x))\n",
        "\n",
        "#         # Get output from Bayesian linear regression\n",
        "#         y_dist = self.bayesian_output.predict_dist(x)\n",
        "\n",
        "#         if self.min_val or self.max_val:\n",
        "#             y_mean = torch.clamp(y_dist.mean, min=self.min_val, max=self.max_val)\n",
        "#         else:\n",
        "#             y_mean = y_dist.mean\n",
        "\n",
        "#         if self.clipping:\n",
        "#             y_mean = y_mean.clamp(min=-1e6, max=1e6)\n",
        "#             y_stddev = y_dist.stddev.clamp(min=1e-6, max=1e1)\n",
        "#         else:\n",
        "#             y_stddev = y_dist.stddev\n",
        "\n",
        "#         return Normal(y_mean, y_stddev)\n",
        "\n",
        "# # Model class using Bayesian MLP\n",
        "# class BayesianMLPModel(Model):\n",
        "#     def __init__(self, train_X, train_Y, min_val=None, max_val=None, clipping=False):\n",
        "#         super().__init__()\n",
        "#         self.bayesian_mlp = BayesianMLP(train_X.shape[1], min_val, max_val, clipping=clipping, hidden_unit_size=64 * 20)\n",
        "#         self.likelihood = GaussianLikelihood()\n",
        "#         self._num_outputs = 1\n",
        "#         self._train_inputs = train_X.to(train_X.device)  # Ensure it's on the right device\n",
        "#         self._train_targets = train_Y.to(train_Y.device)  # Ensure it's on the right device\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.bayesian_mlp(x.to(x.device))\n",
        "\n",
        "#     def posterior(self, X, observation_noise=False, **kwargs):\n",
        "#         pred_dist = self.bayesian_mlp(X.to(X.device))\n",
        "#         mean = pred_dist.mean.squeeze(-1)  # Ensure mean is 2D\n",
        "#         stddev = pred_dist.stddev.squeeze(-1)  # Ensure stddev is 2D\n",
        "#         covar = torch.diag_embed(stddev**2)\n",
        "#         return MultivariateNormal(mean, covar)\n",
        "\n",
        "#     @property\n",
        "#     def num_outputs(self):\n",
        "#         return self._num_outputs\n",
        "\n",
        "#     @property\n",
        "#     def train_inputs(self):\n",
        "#         return self._train_inputs\n",
        "\n",
        "#     @property\n",
        "#     def train_targets(self):\n",
        "#         return self._train_targets\n",
        "\n",
        "# # Function to train the model with GPU support\n",
        "# def fit_pytorch_model(model, num_epochs=1000, learning_rate=0.01):\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#     model.train()\n",
        "#     for epoch in range(num_epochs):\n",
        "#         # print(f\"epoch: {epoch}\")\n",
        "#         optimizer.zero_grad()\n",
        "#         loss = -model(model.train_inputs).log_prob(model.train_targets).mean()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.normal import Normal\n",
        "from botorch.models.model import Model\n",
        "from gpytorch.likelihoods import GaussianLikelihood\n",
        "from gpytorch.distributions import MultivariateNormal\n",
        "\n",
        "# Bayesian Linear Regression class\n",
        "class BayesianLinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(BayesianLinearRegression, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Parameters for prior distributions of weights and biases\n",
        "        self.w_mu = nn.Parameter(torch.zeros(input_dim, output_dim))\n",
        "        self.w_log_sigma = nn.Parameter(torch.zeros(input_dim, output_dim))\n",
        "        self.b_mu = nn.Parameter(torch.zeros(output_dim))\n",
        "        self.b_log_sigma = nn.Parameter(torch.zeros(output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        w_sigma = torch.exp(self.w_log_sigma)\n",
        "        b_sigma = torch.exp(self.b_log_sigma)\n",
        "\n",
        "        # Sample weights and biases\n",
        "        w = self.w_mu + w_sigma * torch.randn_like(self.w_mu)\n",
        "        b = self.b_mu + b_sigma * torch.randn_like(self.b_mu)\n",
        "\n",
        "        return torch.matmul(x, w) + b\n",
        "\n",
        "    def predict_dist(self, x):\n",
        "        y = self.forward(x)\n",
        "\n",
        "        # Compute uncertainty in the output\n",
        "        w_sigma = torch.exp(self.w_log_sigma)\n",
        "        b_sigma = torch.exp(self.b_log_sigma)\n",
        "\n",
        "        # Calculate the standard deviation considering the uncertainty in weights and biases\n",
        "        output_sigma = torch.sqrt(torch.matmul(x**2, w_sigma**2) + b_sigma**2)\n",
        "\n",
        "        return Normal(y, output_sigma)\n",
        "\n",
        "\n",
        "# Bayesian MLP class with adjustable hidden units and layers\n",
        "class BayesianMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_unit_size=64, num_hidden_layers=3, min_val=None, max_val=None, clipping=False):\n",
        "        super(BayesianMLP, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_dim, hidden_unit_size))\n",
        "        layers.append(nn.ReLU())\n",
        "\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            layers.append(nn.Linear(hidden_unit_size, hidden_unit_size))\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "        self.hidden_layers = nn.Sequential(*layers)\n",
        "        self.bayesian_output = BayesianLinearRegression(hidden_unit_size, 1)\n",
        "        self.min_val = min_val\n",
        "        self.max_val = max_val\n",
        "        self.clipping = clipping\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden_layers(x)\n",
        "\n",
        "        # Get output from Bayesian linear regression\n",
        "        y_dist = self.bayesian_output.predict_dist(x)\n",
        "\n",
        "        if self.min_val or self.max_val:\n",
        "            y_mean = torch.clamp(y_dist.mean, min=self.min_val, max=self.max_val)\n",
        "        else:\n",
        "            y_mean = y_dist.mean\n",
        "\n",
        "        if self.clipping:\n",
        "            y_mean = y_mean.clamp(min=-1e6, max=1e6)\n",
        "            y_stddev = y_dist.stddev.clamp(min=1e-6, max=1e1)\n",
        "        else:\n",
        "            y_stddev = y_dist.stddev\n",
        "\n",
        "        return Normal(y_mean, y_stddev)\n",
        "\n",
        "# Model class using Bayesian MLP with adjustable hidden units and layers\n",
        "class BayesianMLPModel(Model):\n",
        "    def __init__(self, train_X, train_Y, hidden_unit_size=64, num_hidden_layers=3, min_val=None, max_val=None, clipping=False):\n",
        "        super().__init__()\n",
        "        self.bayesian_mlp = BayesianMLP(\n",
        "            input_dim=train_X.shape[1], \n",
        "            hidden_unit_size=hidden_unit_size, \n",
        "            num_hidden_layers=num_hidden_layers, \n",
        "            min_val=min_val, \n",
        "            max_val=max_val, \n",
        "            clipping=clipping\n",
        "        )\n",
        "        self.likelihood = GaussianLikelihood()\n",
        "        self._num_outputs = 1\n",
        "        self._train_inputs = train_X.to(train_X.device)  # Ensure it's on the right device\n",
        "        self._train_targets = train_Y.to(train_Y.device)  # Ensure it's on the right device\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bayesian_mlp(x.to(x.device))\n",
        "\n",
        "    def posterior(self, X, observation_noise=False, **kwargs):\n",
        "        pred_dist = self.bayesian_mlp(X.to(X.device))\n",
        "        mean = pred_dist.mean.squeeze(-1)  # Ensure mean is 2D\n",
        "        stddev = pred_dist.stddev.squeeze(-1)  # Ensure stddev is 2D\n",
        "        covar = torch.diag_embed(stddev**2)\n",
        "        return MultivariateNormal(mean, covar)\n",
        "\n",
        "    @property\n",
        "    def num_outputs(self):\n",
        "        return self._num_outputs\n",
        "\n",
        "    @property\n",
        "    def train_inputs(self):\n",
        "        return self._train_inputs\n",
        "\n",
        "    @property\n",
        "    def train_targets(self):\n",
        "        return self._train_targets\n",
        "    \n",
        "\n",
        "# Function to train the model with GPU support\n",
        "def fit_pytorch_model(model, num_epochs=1000, learning_rate=0.01):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        # print(f\"epoch: {epoch}\")\n",
        "        optimizer.zero_grad()\n",
        "        loss = -model(model.train_inputs).log_prob(model.train_targets).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDiTQyUZ6XqW",
        "outputId": "aa1db4c4-6fad-4d06-e788-039840c14857"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on device: cpu\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Optimization completed.\n",
            "Optimal solution: \n",
            "tensor([[ 0., -3., -3., -2.],\n",
            "        [-1., -1.,  0.,  3.],\n",
            "        [-3.,  3., -3., -3.]]), \n",
            "Function value: -3.8000001907348633\n",
            "Total time on cpu: 4.562145948410034 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import warnings\n",
        "import torch\n",
        "from botorch.optim import optimize_acqf\n",
        "from botorch.acquisition import UpperConfidenceBound\n",
        "# from src.bnn import BayesianMLPModel\n",
        "# from src.bnn import fit_pytorch_model\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Function to run the optimization\n",
        "def run_optimization(device):\n",
        "    print(f\"Running on device: {device}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Step 1: Define the Objective Function\n",
        "    weight_matrix = torch.Tensor([\n",
        "        [0.1, 0.4, 0.8, 0.8],\n",
        "        [0.2, 0.4, 0.4, 0.8],\n",
        "        [0.8, 0.1, 0.1, 0.2],\n",
        "    ]).to(device)\n",
        "\n",
        "    objective_function = WarcraftObjectiveBoTorch(weight_matrix=weight_matrix)\n",
        "\n",
        "    # Step 2: Generate Initial Data\n",
        "    X_train, y_train = generate_initial_data(objective_function, 10, weight_matrix.shape)\n",
        "\n",
        "    # Flatten X_train and move it to the correct device\n",
        "    n_samples = X_train.shape[0]\n",
        "    X_train_flat = X_train.view(n_samples, -1).float().to(device)\n",
        "    y_train = y_train.to(device)\n",
        "\n",
        "    # Step 3: Train the Bayesian MLP Model\n",
        "    model = BayesianMLPModel(X_train_flat, y_train, hidden_unit_size=128, num_hidden_layers=5).to(device)\n",
        "    fit_pytorch_model(model, num_epochs=1000, learning_rate=0.01)\n",
        "\n",
        "    # Repeat optimization for a specified number of iterations\n",
        "    n_iterations = 5\n",
        "\n",
        "    for iteration in range(n_iterations):\n",
        "        print(f\"Iteration {iteration + 1}/{n_iterations}\")\n",
        "        # Step 4: Define the Acquisition Function\n",
        "        ucb = UpperConfidenceBound(model, beta=0.1)\n",
        "\n",
        "        # Step 5: Optimize the Acquisition Function\n",
        "        candidate_flat, acq_value = optimize_acqf(\n",
        "            acq_function=ucb,\n",
        "            bounds=torch.tensor([[-3.0] * X_train_flat.shape[1], [3.0] * X_train_flat.shape[1]]).to(device),\n",
        "            q=1,\n",
        "            num_restarts=5,\n",
        "            raw_samples=20,\n",
        "        )\n",
        "\n",
        "        candidate_flat = torch.round(candidate_flat).to(device)\n",
        "        min_key, max_key = -3, 3\n",
        "        candidate_flat = torch.clamp(candidate_flat, min=min_key, max=max_key)\n",
        "        candidate = candidate_flat.view(weight_matrix.shape).to(device)\n",
        "        y_new = objective_function(candidate).to(device)\n",
        "\n",
        "        # Update the Model\n",
        "        X_train = torch.cat([X_train.to(device), candidate.unsqueeze(0).to(device)])\n",
        "        y_train = torch.cat([y_train.to(device), y_new.unsqueeze(0).unsqueeze(-1).to(device)])\n",
        "        X_train_flat = X_train.view(X_train.shape[0], -1).float().to(device)\n",
        "\n",
        "        # Refit the Bayesian MLP model\n",
        "        model = BayesianMLPModel(X_train_flat, y_train, hidden_unit_size=128, num_hidden_layers=5).to(device)\n",
        "        fit_pytorch_model(model, num_epochs=1000, learning_rate=0.01)\n",
        "\n",
        "    print(\"Optimization completed.\")\n",
        "    optim_idx = y_train.argmax()\n",
        "    print(f'Optimal solution: \\n{X_train[optim_idx]}, \\nFunction value: {y_train[optim_idx].item()}')\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Total time on {device}: {end_time - start_time} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "# # Run on GPU\n",
        "# if torch.cuda.is_available():\n",
        "#     run_optimization(torch.device(\"cuda\"))\n",
        "# else:\n",
        "#     print(\"GPU not available.\")\n",
        "\n",
        "# Run on CPU\n",
        "run_optimization(torch.device(\"cpu\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iQV8X7wm6XvQ"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mhidden_layers\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuKgbuLf6X0W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ4mlUlE6X3L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSVJS4nc6X5u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD2_qLI26X8k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i92oJxW16X-4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35YAcu6c6YB-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaVjnjzr6YE_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "Colaboratory へようこそ",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
